
@misc{CScheidegger2013,
  title = {Nearest {{Neighbor Algorithm}} for {{Circular}} Dimensions},
  author = {Scheidegger, Carlos},
  year = {2013},
  journal = {Cross Validated},
  howpublished = {https://stats.stackexchange.com/questions/51908/nearest-neighbor-algorithm-for-circular-dimensions},
  file = {/home/peterprescott/Zotero/storage/XIDH2WMF/nearest-neighbor-algorithm-for-circular-dimensions.html}
}

@misc{D.W.2015,
  title = {Data Structures - {{Find}} k Nearest Neighbors on a Sphere},
  author = {D.W.},
  year = {2015},
  journal = {Computer Science Stack Exchange},
  howpublished = {https://cs.stackexchange.com/questions/48128/find-k-nearest-neighbors-on-a-sphere},
  file = {/home/peterprescott/Zotero/storage/2VDZZ54R/find-k-nearest-neighbors-on-a-sphere.html}
}

@misc{KWeinberger2021,
  title = {{{CS}} 4780 | {{Lecture}} 16: {{KD Trees}}},
  author = {Weinberger, Kilian},
  year = {2021},
  publisher = {{Cornell}},
  file = {/home/peterprescott/Zotero/storage/UJMEBM6K/lecturenote16.html}
}

@misc{LStrous2018,
  title = {Spherical Geometry - Compute Minimum Distance between Point and Great Arc on Sphere},
  author = {Strous, Louis},
  year = {2018},
  journal = {Mathematics Stack Exchange},
  howpublished = {https://math.stackexchange.com/questions/337055/compute-minimum-distance-between-point-and-great-arc-on-sphere},
  file = {/home/peterprescott/Zotero/storage/4QGNV3C6/compute-minimum-distance-between-point-and-great-arc-on-sphere.html}
}

@article{MSkrodzki2019,
  title = {The K-d Tree Data Structure and a Proof for Neighborhood Computation in Expected Logarithmic Time},
  author = {Skrodzki, Martin},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.04936 [cs]},
  eprint = {1903.04936},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {For practical applications, any neighborhood concept imposed on a finite point set P is not of any use if it cannot be computed efficiently. Thus, in this paper, we give an introduction to the data structure of k-d trees, first presented by Friedman, Bentley, and Finkel in 1977. After a short introduction to the data structure (Section 1), we turn to the proof of efficiency by Friedman and his colleagues (Section 2). The main contribution of this paper is the translation of the proof of Freedman, Bentley, and Finkel into modern terms and the elaboration of the proof.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/peterprescott/Zotero/storage/H8IMVLKP/Skrodzki - 2019 - The k-d tree data structure and a proof for neighb.pdf;/home/peterprescott/Zotero/storage/ATB25DCM/1903.html}
}

@misc{Tsoding2017,
  title = {K-d {{Tree}} in {{Python}} \#1 \textemdash{} {{NNS Problem}} and {{Parsing SVG}}},
  author = {{Tsoding}},
  year = {2017},
  month = feb,
  abstract = {Music: http://www.bensound.com/ Source code and SVG file: https://github.com/tsoding/kdtree-in-...}
}

@article{Wikipedia2021,
  title = {Nearest Neighbor Search},
  author = {Wikipedia},
  year = {2021},
  month = jul,
  journal = {Wikipedia},
  abstract = {Nearest neighbor search (NNS), as a form of proximity search,  is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values.  Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q {$\in$} M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points. Most commonly M is a  metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1031642616},
  file = {/home/peterprescott/Zotero/storage/FHIHQE4Y/index.html}
}


