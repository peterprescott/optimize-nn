
@misc{CScheidegger2013,
  title = {Nearest {{Neighbor Algorithm}} for {{Circular}} Dimensions},
  author = {Scheidegger, Carlos},
  year = {2013},
  journal = {Cross Validated},
  howpublished = {https://stats.stackexchange.com/questions/51908/nearest-neighbor-algorithm-for-circular-dimensions},
  file = {/home/peterprescott/Zotero/storage/XIDH2WMF/nearest-neighbor-algorithm-for-circular-dimensions.html}
}

@misc{D.W.2015,
  title = {Data Structures - {{Find}} k Nearest Neighbors on a Sphere},
  author = {D.W.},
  year = {2015},
  journal = {Computer Science Stack Exchange},
  howpublished = {https://cs.stackexchange.com/questions/48128/find-k-nearest-neighbors-on-a-sphere},
  file = {/home/peterprescott/Zotero/storage/2VDZZ54R/find-k-nearest-neighbors-on-a-sphere.html}
}

@inproceedings{JVanderPlasEtAl2012,
  title = {Introduction to {{astroML}}: Machine Learning for Astrophysics},
  shorttitle = {Introduction to {{astroML}}},
  booktitle = {2012 {{Conference}} on {{Intelligent Data Understanding}}},
  author = {VanderPlas, Jacob and Connolly, Andrew J. and Ivezi{\'c}, {\v Z}eljko and Gray, Alex},
  year = {2012},
  month = oct,
  pages = {47--54},
  doi = {10.1109/CIDU.2012.6382200},
  abstract = {Astronomy and astrophysics are witnessing dramatic increases in data volume as detectors, telescopes and computers become ever more powerful. During the last decade, sky surveys across the electromagnetic spectrum have collected hundreds of terabytes of astronomical data for hundreds of millions of sources. Over the next decade, the data volume will enter the petabyte domain, and provide accurate measurements for billions of sources. Astronomy and physics students are not traditionally trained to handle such voluminous and complex data sets. In this paper we describe astroML; an initiative, based on python and scikit-learn, to develop a compendium of machine learning tools designed to address the statistical needs of the next generation of students and astronomical surveys. We introduce astroML and present a number of example applications that are enabled by this package.},
  keywords = {Astrophysics,Density measurement,Energy measurement,Extraterrestrial measurements,Noise,Object recognition},
  file = {/home/peterprescott/Zotero/storage/MWFY2R9T/VanderPlas et al. - 2012 - Introduction to astroML Machine learning for astr.pdf}
}

@misc{KWeinberger2021,
  title = {{{CS}} 4780 | {{Lecture}} 16: {{KD Trees}}},
  author = {Weinberger, Kilian},
  year = {2021},
  publisher = {{Cornell}},
  file = {/home/peterprescott/Zotero/storage/UJMEBM6K/lecturenote16.html}
}

@misc{LStrous2018,
  title = {Spherical Geometry - Compute Minimum Distance between Point and Great Arc on Sphere},
  author = {Strous, Louis},
  year = {2018},
  journal = {Mathematics Stack Exchange},
  howpublished = {https://math.stackexchange.com/questions/337055/compute-minimum-distance-between-point-and-great-arc-on-sphere},
  file = {/home/peterprescott/Zotero/storage/4QGNV3C6/compute-minimum-distance-between-point-and-great-arc-on-sphere.html}
}

@article{MSkrodzki2019,
  title = {The K-d Tree Data Structure and a Proof for Neighborhood Computation in Expected Logarithmic Time},
  author = {Skrodzki, Martin},
  year = {2019},
  month = mar,
  journal = {arXiv:1903.04936 [cs]},
  eprint = {1903.04936},
  eprinttype = {arxiv},
  primaryclass = {cs},
  abstract = {For practical applications, any neighborhood concept imposed on a finite point set P is not of any use if it cannot be computed efficiently. Thus, in this paper, we give an introduction to the data structure of k-d trees, first presented by Friedman, Bentley, and Finkel in 1977. After a short introduction to the data structure (Section 1), we turn to the proof of efficiency by Friedman and his colleagues (Section 2). The main contribution of this paper is the translation of the proof of Freedman, Bentley, and Finkel into modern terms and the elaboration of the proof.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Data Structures and Algorithms},
  file = {/home/peterprescott/Zotero/storage/H8IMVLKP/Skrodzki - 2019 - The k-d tree data structure and a proof for neighb.pdf;/home/peterprescott/Zotero/storage/ATB25DCM/1903.html}
}

@misc{Tsoding2017,
  title = {K-d {{Tree}} in {{Python}} \#1 \textemdash{} {{NNS Problem}} and {{Parsing SVG}}},
  author = {{Tsoding}},
  year = {2017},
  month = feb,
  abstract = {Music: http://www.bensound.com/ Source code and SVG file: https://github.com/tsoding/kdtree-in-...}
}

@article{Wikipedia2021,
  title = {Nearest Neighbor Search},
  author = {Wikipedia},
  year = {2021},
  month = jul,
  journal = {Wikipedia},
  abstract = {Nearest neighbor search (NNS), as a form of proximity search,  is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values.  Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q {$\in$} M, find the closest point in S to q. Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. A direct generalization of this problem is a k-NN search, where we need to find the k closest points. Most commonly M is a  metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold.},
  copyright = {Creative Commons Attribution-ShareAlike License},
  langid = {english},
  annotation = {Page Version ID: 1031642616},
  file = {/home/peterprescott/Zotero/storage/FHIHQE4Y/index.html}
}


